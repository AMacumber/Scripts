{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define ETL methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table_to_df(tablename, db_engine):\n",
    "    return pd.read_swl(\"SELECT * FROM {}\".format(tablename), db_engine)\n",
    "\n",
    "def split_columns_transform(df, column, pat, suffixes):\n",
    "    # converts column into str and splits it on pat..\n",
    "\n",
    "def load_df_into_dwh(film_df, tablename, schema, db_engine):\n",
    "    return pd.to_sql(tablename, db_engine, schema = schema, if_exists = \"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_uri = \"postgresql://[user]:[password]@[host]:[port][/database]\"\n",
    "db_engine = sqlalchemy.create_engine(connection_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the ETL Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl():\n",
    "    df = extract_table_to_df()\n",
    "    df = split_columns_transform(df)\n",
    "    load_df_into_dwh(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "\n",
    "dag = DAG(dag_id = \"etl_pipeline\",\n",
    "         schedule_interval = \"0 0 * * *\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the ETL task using PythonOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "etl_task = PythonOperator(task_id = 'etl_task',\n",
    "                         python_callable = etl,\n",
    "                         dag = dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the upstream or downstream dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl_task.set_upstream(wait_for_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the ETL function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once completed, you can save it as a python file:\n",
    "\n",
    "*etl_dag.py*\n",
    "\n",
    "This can then be saved in the airflow dag folder:\n",
    "\n",
    "*~/airflow/dags/*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
